<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>Publication</title>
		<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"/>
		<meta name="keywords" content="S2Lab、内蒙古大学、内蒙古大学计算机学院、刘瑞研究组、语音理解与合成" />
		<meta name="description" content="内蒙古大学计算机学院刘瑞研究员科研团队">
		<link type="text/css" href="./font-awesome/css/font-awesome.min.css" rel="stylesheet">
		<link type="text/css" href="./css/basic.css" rel="stylesheet">
		<link type="text/css" href="./css/header.css" rel="stylesheet">
		<link type="text/css" href="./css/pub.css" rel="stylesheet">
		<link type="text/css" href="./css/fooder.css" rel="stylesheet">
		<script type='text/javascript' src="./js/jquery-3.3.1.min.js"></script>
		
		
	</head>
	<body>
		<header class="pc_header">
			<div class="header_left">
				<img src="img/title.png"/>
				<!-- <span>语音生成与理解研究组</span> -->
			</div>
			<div class="header_right">
				<ul>
					<li id="element_1" onclick="window.location.href='./index.html'">
						<a>Home</a>
					</li>
					<li id="element_2" onclick="window.location.href='./people.html'">
						<a>People</a>
					</li>
					<li id="element_3" style="color: #015293;">
						<i class="fa fa-user" aria-hidden="true"></i>
						<a>Publications</a>
					</li>

					
					<li id="element_5" onclick="window.location.href='./activity.html'">
						<a>Activity</a>
					</li>
					
					<li id="element_4" onclick="window.location.href='./joinUs.html'">
						<a>Join Us</a>
					</li>
					
				</ul>
			</div>
		</header>
		<main class="pc_pub">
			<span class="pub_title">Journal and Conference</span>


			<!-- 2025 -->
			<div class="pub_year">2025</div>
			<div class="pub_list">
				<div class="pub_seg">Conference</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/Chain-Talker.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Chain-Talker: Chain Understanding and Rendering for Empathetic Conversational Speech Synthesis</div>
						<div class="paper_author">Yifan Hu, <strong>Rui Liu *</strong>, Yi Ren, Xiang Yin, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/xxx" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/xxx.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/xxx" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ACL 2025
					</div></div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/AffectGPT.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models</div>
						<div class="paper_author">Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, <strong>Rui Liu</strong>, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2501.16566" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ICML2025-AffectGPT.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/xxx" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICML 2025
					</div></div>




				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/OV-MER.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition</div>
						<div class="paper_author">Zheng Lian, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun Chen, Zhang Siyuan, Hailiang Yao, Bin Liu, <strong>Rui Liu</strong>, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2412.11409" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ICML2025-OV-MER.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/xxx" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICML 2025
					</div></div>

				
				
					<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/M2SE-VTTS.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Multi-modal and Multi-scale Spatial Environment Understanding for Immersive Visual Text-to-Speech</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Shuwei He, Yifan Hu, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2412.11409" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/AAAI2025.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/M2SE-VTTS" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>AAAI 2025
					</div></div>




					<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/MS2KU-VTTS.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Multi-Source Spatial Knowledge Understanding for Immersive Visual Text-to-Speech</div>
						<div class="paper_author">Shuwei He, <strong>Rui Liu *</strong>.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10888876" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ICASSP2025-MS2KU-VTTS.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/MS2KU-VTTS" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2025
					</div></div>


					<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/I3CSS.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Intra- and Inter-modal Context Interaction Modeling for Conversational Speech Synthesis</div>
						<div class="paper_author">Zhenqi Jia, <strong>Rui Liu *</strong>.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10890216" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ICASSP2025-I3CSS.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/I3CSS" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2025
					</div></div>


					
					<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/M2CI-Dubber.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Towards Expressive Video Dubbing with Multiscale Multimodal Context Interaction</div>
						<div class="paper_author">Yuan Zhao, <strong>Rui Liu *</strong>, Gaoxiang Cong.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10887942" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ICASSP2025-M2CI-Dubber.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/M2CI-Dubber" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2025
					</div></div>



				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/MEIJU.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MEIJU-The 1st Multimodal Emotion and Intent Joint Understanding Challenge</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Xiaofeng Xing, Zheng Lian, Haizhou Li, Björn W. Schuller, Haolin Zuo</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10890352" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ICASSP2025-MEIJU.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://ai-s2-lab.github.io/MEIJU2025-website/" target='_blank' style="color: #015293;">[Website]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2025
					</div></div>





				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/EmoCorrector.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Pu Gao, Jiatian Xi, Berrak Sisman, Carlos Busso, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="xxx" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/xxx.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/EmoCorrector" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>INTERSPEECH 2025
					</div></div>

  
				
				<!-- 期刊 -->
				<div class="pub_seg">Journal</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/NCE-TTS2025.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Noise Robust Cross-Speaker Emotion Transfer in TTS through Knowledge Distillation and Orthogonal Constraint</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Kailin Liang, De Hu, Tao Li, Dongchao Yang, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10852404" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ssmlkl/NCE-TTS-Demo" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE Transactions on Audio, Speech and Language Processing (IEEE-TASLP) 2025
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/RADKA-CSS.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Retrieval-Augmented Dialogue Knowledge Aggregation for Expressive Conversational Speech Synthesis</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Zhenqi Jia, Feilong Bao *, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2501.06467" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/Coder-jzq/RADKA-CSS" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Information Fusion (INFFUS) 2025
					</div>
				</div>



				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/HMSCF-ADD.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Hierarchical Multi-Source Cues Fusion for Mono-to-Binaural based Audio Deepfake Detection</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Jinhua Zhang, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/xxx" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/AI-S2-Lab/HMSCF-ADD" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Information Fusion (INFFUS) 2025
					</div>
				</div>




				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/AVGER.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Listening and Seeing Again: Generative Error Correction for Audio-Visual Speech Recognition</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Hongyu Yuan, Guanglai Gao, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2501.04038" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/CircleRedRain/AVGER" target='_blank' style="color: #015293;">[CODE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Information Fusion (INFFUS) 2025
					</div>
				</div>



				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/RSL-PSL.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Robust Self-Localization of Wireless Acoustic Sensor Networks</div>
						<div class="paper_author">Xu Wang, De Hu, <strong>Rui Liu</strong>, Feilong Bao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10915210" target='_blank' style="color: #015293;">[PAPER]</a>
<!-- 							<a class="paper_code" href="https://github.com/CircleRedRain/AVGER" target='_blank' style="color: #015293;">[CODE]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE Internet of Things Journal (IEEE-IoTJ) 2025
					</div>
				</div>



				


			

 
			</div>
			
			<!-- 2024 -->
			<div class="pub_year">2024</div>
			<div class="pub_list">
				<div class="pub_seg">Conference</div>




				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/GPT-Talker.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Generative Expressive Conversational Speech Synthesis</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2407.21491" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/ACMMM2024.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/GPT-Talker" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ACMMM 2024
					</div>
				</div>

				

				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ECSS.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ojs.aaai.org/index.php/AAAI/article/view/29833" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="papers/AAAI2024.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/walker-hyf/ECSS" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>AAAI 2024
					</div>
				</div>




				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/MRAC24.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MRAC'24 Track 2: 2nd International Workshop on Multimodal and Responsible Affective Computing</div>
						<div class="paper_author">Zheng Lian, Bin Liu, <strong>Rui Liu</strong>, Kele Xu, Erik Cambria, Guoying Zhao, Björn W Schuller, Jianhua Tao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://dl.acm.org/doi/10.1145/3689092.3696103" target='_blank' style="color: #015293;">[PAPER]</a>
<!-- 							<a class="paper_pdf" href="papers/AAAI2024.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/walker-hyf/ECSS" target='_blank' style="color: #015293;">[CODE & DEMO]</a> -->
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>MRAC'24 Workshop (ACMMM 2024)  
					</div>
				</div>

 

				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/MER2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition</div>
						<div class="paper_author">Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, <strong>Rui Liu</strong>, Kele Xu, Bin Liu, Erik Cambria, Guoying Zhao, Björn W. Schuller, Jianhua Tao .</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://dl.acm.org/doi/10.1145/3689092.3689411" target='_blank' style="color: #015293;">[PAPER]</a>
<!-- 							<a class="paper_pdf" href="papers/AAAI2024.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/walker-hyf/ECSS" target='_blank' style="color: #015293;">[CODE & DEMO]</a> -->
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>MRAC'24 Workshop (ACMMM 2024)  
					</div>
				</div>
				


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/NMER.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Learning Noise-Robust Joint Representation for Multimodal Emotion Recognition under Incomplete Data Scenarios</div>
						<div class="paper_author">Qi Fan, Haolin Zuo, <strong>Rui Liu *</strong>, Zheng Lian, Guanglai Gao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://dl.acm.org/doi/10.1145/3689092.3689411" target='_blank' style="color: #015293;">[PAPER]</a>
<!-- 							<a class="paper_pdf" href="papers/AAAI2024.txt" target='_blank' style="color: #015293;">[BIB]</a>
							<a class="paper_pdf" href="https://github.com/walker-hyf/ECSS" target='_blank' style="color: #015293;">[CODE & DEMO]</a> -->
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>MRAC'24 Workshop (ACMMM 2024)  
					</div>
				</div>

				

				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/IJCNN2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Pre-training Language Model for Mongolian with Agglutinative Linguistic Knowledge Injection</div>
						<div class="paper_author">Muhan Na, <strong>Rui Liu *</strong>, Feilong Bao, Guanglai Gao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="xxx" target='_blank' style="color: #015293;">[PAPER]</a>
							<!-- <a class="paper_pdf" href="https://github.com/walker-hyf/ECSS" target='_blank' style="color: #015293;">[CODE & DEMO]</a>
							<a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IJCNN 2024
					</div>
				</div>



				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/InterSpeech2024-mzn.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Emotion-Aware Speech Self-Supervised Representation Learning with Intensity Knowledge</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Zening Ma.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://www.arxiv.org/abs/2406.06646" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/EMS" target='_blank' style="color: #015293;">[CODE]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>InterSpeech 2024
					</div>
				</div>



				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/InterSpeech2024-xjt.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Jiatian Xi, Ziyue Jiang, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2309.11725" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/FluentEditor" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="https://ai-s2-lab.github.io/FluentEditor/" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>InterSpeech 2024
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ISCSLP2024-FCTalker.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis</div>
						<div class="paper_author">Yifan Hu, <strong>Rui Liu *</strong>, Guanglai Gao, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/pdf/2210.15360" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://github.com/walker-hyf/FCTalker" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="https://walker-hyf.github.io/FCTalker/" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ISCSLP 2024
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ICAGC2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024</div>
						<div class="paper_author">Ruibo Fu, <strong>Rui Liu</strong>, Chunyu Qiang, Yingming Gao, Yi Lu, Shuchen Shi, Tao Wang, Ya Li, Zhengqi Wen, Chen Zhang, Hui Bu, Yukun Liu, Xin Qi, Guanjun Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10800374" target='_blank' style="color: #015293;">[PAPER]</a>
<!-- 							<a class="paper_pdf" href="https://github.com/walker-hyf/FCTalker" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="https://walker-hyf.github.io/FCTalker/" target='_blank' style="color: #015293;">[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ISCSLP 2024
					</div>
				</div>



				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/NCMMSC2024-MCDubber.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MCDubber: Multimodal Context-Aware Expressive Video Dubbing</div>
						<div class="paper_author">Yuan Zhao，Zhenqi Jia，<strong>Rui Liu *</strong>，De Hu，Feilong Bao，Guanglai Gao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/pdf/xx" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://github.com/XiaoYuanJun-zy/MCDubber" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="https://github.com/XiaoYuanJun-zy/MCDubber" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>NCMMSC 2024
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/NCMMSC2024-EmoPP.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech</div>
						<div class="paper_author">Bin Liu，<strong>Rui Liu *</strong>，Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/pdf/xx" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/EmoPP" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="https://ai-s2-lab.github.io/EmoPP/" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>NCMMSC 2024
					</div>
				</div>
 
 
				
				<!-- 期刊 -->
				<div class="pub_seg">Journal</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/TAC2024SQY.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Connecting Cross-Modal Representations for Compact and Robust Multimodal Sentiment Analysis With Sentiment Word Substitution Error</div>
						<div class="paper_author">Qiyuan Sun, Haolin Zuo, <strong>Rui Liu *</strong>, and Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/abstract/document/10741889" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/AI-S2-Lab/ARF-MSA" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE Transactions on Affective Computing (IEEE-TAFFC) 2024
					</div>
				</div>



				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/AccentTTS2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Controllable Accented Text-to-Speech Synthesis with Fine and Coarse-Grained Intensity Rendering</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Guanglai Gao and Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10487819" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://ttslr.github.io/CTA-TTS/" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP)  2024
					</div>
				</div>


				

				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/TAC2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Haolin Zuo, Zheng Lian, Bjorn W. Schuller and Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10474146/" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ZhuoYulang/CIF-MMIN" target='_blank' style="color: #015293;">[CODE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE Transactions on Affective Computing (IEEE-TAFFC) 2024
					</div>
				</div>

				
				 
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/IF2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Multi-Space Channel Representation Learning for Mono-to-Binaural Conversion based Audio Deepfake Detection</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Jinhua Zhang and Guanglai Gao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://www.sciencedirect.com/science/article/pii/S1566253524000356" target='_blank' style="color: #015293;">[PAPER]</a>
							<!-- <a class="paper_code" href="xxx" target='_blank' style="color: #015293;">[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Information Fusion (INFFUS) 2024
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/taslp2024.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Text-to-Speech for Low-Resource Agglutinative Language with Morphology-Aware Language Model Pre-training</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Yifan Hu, Haolin Zuo, Zhaojie Luo, Longbiao Wang and Guanglai Gao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10379131" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://ttslr.github.io/MAM-BERT/" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP) 2024
					</div>
				</div>

			</div>
			


			<!-- 2023 -->
			<div class="pub_year">2023</div>
			<div class="pub_list">
				<div class="pub_seg">Conference</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/LR_INTERSPEECH2023.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Explicit Intensity Control for Accented Text-to-speech</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Haolin Zuo, De Hu, Guanglai Gao, Haizhou Li.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/LR_INTERSPEECH2023.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://ttslr.github.io/Ai-TTS/" target='_blank' style="color: #015293;">[DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>InterSpeech 2023
					</div>
				</div>

				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ZJH_INTERSPEECH2023.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Jinhua Zhang, Guanglai Gao and Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/ZJH_INTERSPEECH2023.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_pdf" href="https://github.com/AI-S2-Lab/M2S-ADD" target='_blank' style="color: #015293;">[CODE]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>InterSpeech 2023
					</div>
				</div>

				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/paper-2.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Exploiting Modality-invariant Feature For Robust Multimodal Eemotion Recognition With Missing Modalities</div>
						<div class="paper_author">Haolin Zuo, <strong>Rui Liu</strong><sup>*</sup>, Jinming Zhao, Guanglai Gao, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2210.15359" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ZhuoYulang/IF-MMIN" target='_blank' style="color: #015293;">[CODE]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2023
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/MonAD.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Research on Mongolian Synthetic Speech Detection based on Deep Learning (in Chinese)</div>
						<div class="paper_author">Jinhua Zhang, Kailin Liang, <strong>Rui Liu</strong><sup>*</sup></div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/MonAD-NCMMSC2023.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ssmlkl/NCMMSC2023" target='_blank' style="color: #015293;">[CODE&DEMO]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>NCMMSC 2023
					</div>
				</div>
				
				<!-- 期刊 -->
				<div class="pub_seg">Journal</div>

				 
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/hude-TASLP.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Distributed Sensor Selection for Speech Enhancement with Acoustic Sensor Networks</div>
						<div class="paper_author">De Hu, Qintuya Si, <strong>Rui Liu *</strong>, Feilong Bao.</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/10043687" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ttslr/DSS-SE" target='_blank' style="color: #015293;">[CODE]</a>
							<!-- <a class="paper_demo"  target='_blank'>[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP) 2023
					</div>
				</div>

				<!-- <div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ControllableAccentedText-to-Speech.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Controllable Accented Text-to-Speech Synthesis</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Guanglai Gao, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2209.10804" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://speechdemo.github.io/caitts" target='_blank' style="color: #015293;">[CODE]</a>
							 <a class="paper_demo"  target='_blank'>[DEMO]</a>  
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Under Review
					</div>
				</div> -->
			</div>
			
			<!-- 2022 -->
			<div class="pub_year">2022</div>

			<div class="pub_list">
				<div class="pub_seg">Conference</div>
				


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ICASSP2022-WYH.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Alignment Learning based Single-Step Decoding for Accurate and Fast Non-Autoregressive Speech Recognition</div>
						<div class="paper_author">Yonghe Wang, <strong>Rui Liu</strong><sup>*</sup>, Feilong Bao, Hui Zhang, Guanglai Gao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/ICASSP2022-WYH.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/AccurateEmotionStrength.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Björn Schuller, Guanglai Gao and Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://www.isca-speech.org/archive/interspeech_2022/liu22i_interspeech.html" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ttslr/StrengthNet" target='_blank' style="color: #015293;">[CODE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>INTERSPEECH 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/VisualTTS.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">VisualTTS: TTS with Accurate Lip-speech Synchronization for Automatic Voice Over</div>
						<div class="paper_author">Junchen Lu, Berrak Sisman, <strong>Rui Liu</strong>, Mingyang Zhang, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2110.03342" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_demo" href="https://ranacm.github.io/VisualTTS-Samples/" target='_blank' style="color: #015293;">[DEMO]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>INTERSPEECH 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/paper-1.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset</div>
						<div class="paper_author">Kailin Liang<sup>†</sup>, Bin Liu<sup>†</sup>, Yifan Hu<sup>†</sup>, <strong>Rui Liu</strong><sup>*</sup>, Feilong Bao, and Guanglai Gao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/NCMMSC_2022_LiangKailin_LiuBin.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/ssmlkl/MnTTS2" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="http://mglip.com/corpus/corpus_detail.html?corpusid=20221106113633" target='_blank' style="color: #015293;">[SOURCE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>NCMMSC 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/paper-3.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied Baseline</div>
						<div class="paper_author">Yifan Hu<sup>†</sup>, Pengkai Yin<sup>†</sup>, <strong>Rui Liu</strong><sup>*</sup>, Feilong Bao and Guanglai Gao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2209.10848" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://github.com/walker-hyf/MnTTS" target='_blank' style="color: #015293;">[CODE]</a>
							<a class="paper_demo" href="http://mglip.com/corpus/corpus_detail.html?corpusid=20220819185345" target='_blank' style="color: #015293;">[SOURCE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IALP 2022
					</div>
				</div>


				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/paper-4.png"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">A Deep Investigation of RNN and Self-attention for the Cyrillic-Traditional Mongolian Bidirectional Conversion</div>
						<div class="paper_author">Muhan Na, <strong>Rui Liu</strong><sup>*</sup>, Feilong Bao and Guanglai Gao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2209.11963" target='_blank' style="color: #015293;">[PAPER]</a>
							<!-- <a class="paper_code">[CODE]</a>
							<a class="paper_demo">[DEMO]</a> -->
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICONIP 2022
					</div>
				</div>
				
				
				<!-- 期刊 -->
				<div class="pub_seg">Journal</div>
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/Speaker-Independent.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter Networks</div>
						<div class="paper_author">Zhaojie Luo, Shoufeng Lin, <strong>Rui Liu</strong><sup> *</sup>, Jun Baba, Yuichiro Yoshikawa, Ishiguro Hiroshi</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9829916" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://zhaojiel.github.io/SFEVC/" target='_blank' style="color: #015293;">[CODE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP) 2022
					</div>
				</div>
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/TASLP2022-decoding.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Decoding Knowledge Transfer for Neural Text-to-Speech Training</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Member, IEEE, Berrak Sisman, Member, IEEE, Guanglai Gao, and Haizhou Li, Fellow, IEEE</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/TASLP2022-decoding.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP) 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/Multi-StageDeep.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Multi-Stage Deep Transfer Learning for EmIoT-enabled Human-Computer Interaction</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Qi Liu, Hongxu Zhu, Hui Cao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ieeexplore.ieee.org/document/9702532" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="hhttps://ttslr.github.io/IOT/" target='_blank' style="color: #015293;">[CODE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE Internet of Things Journal (IEEE-IoTJ) 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/MonTTS.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">MonTTS: A Real-time and High-fidelity Mongolian TTS Model with Complete Non-autoregressive Mechanism (in Chinese)</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Shyin Kang, Jingdong Li, Feilong Bao, Guanglai Gao</div>
						<div class="paper_link">
							<a class="paper_pdf" href="http://jcip.cipsc.org.cn/CN/abstract/abstract3357.shtml" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_code" href="https://ttslr.github.io/IOT" target='_blank' style="color: #015293;">[CODE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>中文信息学报 2022
					</div>
				</div>
			</div>
			
			
			<!-- 2021 -->
			<div class="pub_year">2021</div>
			<div class="pub_list">
				<div class="pub_seg">Conference</div>
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ICASSP2021.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">GraphSpeech: Syntax-Aware Graph Attention Network for Neural Speech Synthesis</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/ICASSP2021.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2021
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/InterSpeech2021.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/interspeech2021-ruiliu.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>InterSpeech 2021
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/SEENANDUNSEEN.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Seen and Unseen Emotional Style Transfer for Voice Conversion with a New Emotion Speech Dataset</div>
						<div class="paper_author">Kun Zhou, Berrak Sisman, <strong>Rui Liu</strong>, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/pdf/2010.14794.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2021
					</div>
				</div>
				
				
				
				<!-- 期刊 -->
				<div class="pub_seg">Journal</div>
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/Emotionalvoiceconversion.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Emotional Voice Conversion: Theory, Databases and ESD</div>
						<div class="paper_author">Kun Zhou, Berrak Sisman, <strong>Rui Liu</strong>, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://arxiv.org/abs/2105.14762" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_demo" href="https://hltsingapore.github.io/ESD/demo.html" target='_blank' style="color: #015293;">[SOURCE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Speech Communication (SPECOM) 2022
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/TASLP2021-style.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Expressive TTS Training With Frame and Style Reconstruction Loss</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Member, IEEE, Berrak Sisman, Member, IEEE, Guanglai Gao, and Haizhou Li, Fellow, IEEE</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/TASLP2021-style.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_demo" href="https://ttslr.github.io/Expressive-TTS-Training-with-Frame-and-Style-Reconstruction-Loss/" target='_blank' style="color: #015293;">[SOURCE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP) 2021
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/NN2021.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">FastTalker: A neural text-to-speech architecture with shallow and group autoregression</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Yixing Lin, Haizhou Li,</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/NN2021.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_demo" href="https://ttslr.github.io/FastTalker/" target='_blank' style="color: #015293;">[SOURCE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Neural Networks (NEUNET) 2021
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ExploitingMorphological.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing for Mongolian Speech Synthesis</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Feilong Bao, Jichen Yang, Guanglai Gao, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="https://ttslr.github.io/papers/TASLP2020Mongolian.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
							<a class="paper_demo" href="https://ttslr.github.io/FastTalker/" target='_blank' style="color: #015293;">[SOURCE]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE/ACM Transactions on Audio, Speech and Language Processing (IEEE/ACM-TASLP) 2021
					</div>
				</div>
			</div>
			
			<!-- 2020 -->
			<div class="pub_year">2020</div>
			<div class="pub_list">
				<div class="pub_seg">Conference</div>
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/ICASSP2020.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Teacher-Student Training For Robust Tacotron-Based TTS</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Jingdong Li, Feilong Bao, Guanglai Gao, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/ICASSP2020.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>ICASSP 2020
					</div>
				</div>
				
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/Odyssey2020.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/Odyssey2020.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>Odyssey 2020
					</div>
				</div>
				
				
				
				<!-- 期刊 -->
				<div class="pub_seg">Journal</div>
				<div class="pub_detail">
					<div class="paper_image">
						<img src="img/papersImg/SPL2020.jpg"/>
					</div>
					<div class="paper_left">
						<div class="paper_title">Modeling Prosodic Phrasing With Multi-Task Learning in Tacotron-Based TTS</div>
						<div class="paper_author"><strong>Rui Liu</strong>, Member, IEEE, Berrak Sisman, Member, IEEE, Feilong Bao, Guanglai Gao, and Haizhou Li, Fellow, IEEE</div>
						<div class="paper_link">
							<a class="paper_pdf" href="papers/SPL2020.pdf" target='_blank' style="color: #015293;">[PAPER]</a>
						</div>
					</div>
					<div class="paper_right">
						<i class="fa fa-group"></i><br/>IEEE Signal Processing Letters 2020
					</div>
				</div>
				 
			</div>
			
		</main>
		
		<footer class="pc_footer">
			<div class="element">
				<i class="fa fa-envelope-open-o" aria-hidden="true" /></i>
				<span>liurui_imu @163.com</span>
			</div>
			
			<div class="element">
				<i class="fa fa-phone" aria-hidden="true" /></i>
				<span>+86 16647162610</span>
			</div>
			
			<div class="element">
				<i class="fa fa-map-marker" aria-hidden="true" /></i>
				<span> 503 Room, School of Computer science, <br/>Inner Mongolia University (010021)</span>
			</div>
			
			<div class="element">
				<i class="fa fa-map" aria-hidden="true" /></i>
				<span>Hohhot, China</span>
			</div>
		</footer>
		
	</body>
</html>
