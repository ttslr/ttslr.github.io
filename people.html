<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8" />
		<title>People</title>
		<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no"/>
		<meta name="keywords" content="S2Lab、内蒙古大学、内蒙古大学计算机学院、刘瑞研究组、语音理解与合成" />
		<meta name="description" content="内蒙古大学计算机学院刘瑞研究员科研团队">
		<link type="text/css" href="./font-awesome/css/font-awesome.min.css" rel="stylesheet">
		<link type="text/css" href="./css/basic.css" rel="stylesheet">
		<link type="text/css" href="./css/header.css" rel="stylesheet">
		<link type="text/css" href="./css/people.css" rel="stylesheet">
		<link type="text/css" href="./css/fooder.css" rel="stylesheet">
		<link rel="canonical" href="https://ttslr.github.io/">
		<script type='text/javascript' src="./js/jquery-3.3.1.min.js"></script>
		<link rel="stylesheet" href="./css/main_rl.css">
		<meta http-equiv="cleartype" content="on">
		<link rel="stylesheet" href="./css/academicons_rl.css" />
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
		</script>
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); 
		</script>
		<script>
			document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async>
		</script>
	</head>
	<body>
		<header class="pc_header">
			<div class="header_left">
				<img src="img/title.png"/>
				<!-- <span>语音生成与理解研究组</span> -->
			</div>
			<div class="header_right">
				<ul>
					<li id="element_1" onclick="window.location.href='./index.html'">
						<a>Home</a>
					</li>

					<li id="element_2" style="color: #015293;">
						<i class="fa fa-user" aria-hidden="true"></i>
						<a>People</a>
					</li>
					<li id="element_3" onclick="window.location.href='./publication.html'">
						<a>Publications</a>
					</li>
					
					
					
					<li id="element_5" onclick="window.location.href='./activity.html'">
						<a>Activity</a>
					</li>
					
					<li id="element_4" onclick="window.location.href='./joinUs.html'">
						<a>Join Us</a>
					</li>
				</ul>
			</div>
			
			
		</header>
		
		<main class="pc_people">
			<img src="img/4.jpg"/>
			
			<!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->
			<div class="masthead">
				<div class="masthead__inner-wrap">
					<div class="masthead__menu">
						<nav id="site-nav" class="greedy-nav">
							<button>
								<div class="navicon"></div>
							</button>
							<ul class="visible-links">
								<li class="masthead__menu-item">
									<!-- <a href="https://ttslr.github.io/index_ruiliu.html" onclick="myFunction(1)">Rui Liu 刘瑞</a> -->
									<a href="#" onclick="showModule(1)">Rui Liu (刘瑞)</a>
								</li>
								<li class="masthead__menu-item">
									<!-- <a href="https://ttslr.github.io/" onclick="myFunction(1)">S2LAB 团队网页</a> -->
									<a href="#" onclick="showModule(2)">Team Members</a>
								</li>
								<li class="masthead__menu-item">
									<!-- <a href="https://ttslr.github.io/" onclick="myFunction(2)">合作机构</a> -->
									<a href="#" onclick="showModule(3)">Collaborators</a>
								</li>
							</ul>
							<ul class="hidden-links hidden"></ul>
						</nav>
					</div>
				</div>
			</div>
			<div class="module" id="module1">
			    <div id="main" role="main">
			    	<!-- 左侧信息栏 -->
			    	<div class="sidebar sticky">
			    		<div itemscope itemtype="http://schema.org/Person">
			    			<div class="author__avatar">
			    				<img src="assets/RuiLiu.jpg" class="author__avatar" alt="Hanlei Zhang">
			    			</div>
			    			<div class="author__content">
			    				<h3 class="author__name">Rui Liu</h3>
			    
			    
			    				<p class="author__bio">Speech Information Processing, Natural Language Processing, and
			    					Multimodal Human-Computer Dialogue.</p>
			    				<p class="author__bio">
			    					<!-- Office: 503 Room, School of Computer science, Inner Mongolia University, China 010021 -->
			    					Mobile: +86 16647162610
			    				</p>
			    			</div>
			    			<div class="author__urls-wrapper">
			    				<button class="btn btn--inverse">Follow</button>
			    				<ul class="author__urls social-icons">
			    					<li>
			    						<i class="fa fa-fw fa-map-marker" aria-hidden="true"></i>
			    						Inner Mongolia University, China
			    					</li>
			    					<li>
			    						<a href="mailto:liurui_imu@163.com">
			    							<i class="fas fa-fw fa-envelope" aria-hidden="true"></i>
			    							Email
			    						</a>
			    					</li>
			    					<!-- <li>
			                            <a href="xxx">
			                                <i class="fab fa-fw fa-researchgate" aria-hidden="true"></i>
			                                ResearchGate
			                            </a>
			                        </li> -->
			    					<li>
			    						<a href="https://twitter.com/RuiLiu60711141">
			    							<i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i>
			    							Twitter
			    						</a>
			    					</li>
			    					<li>
			    						<a href="https://github.com/ttslr">
			    							<i class="fab fa-fw fa-github" aria-hidden="true"></i>
			    							Github
			    						</a>
			    					</li>
			    					<li>
			    						<a href="https://scholar.google.com.sg/citations?user=B2t0J-IAAAAJ&hl=zh-CN">
			    							<i class="fas fa-fw fa-graduation-cap"></i>
			    							Google Scholar
			    						</a>
			    					</li>
			    					<!-- <li>
			                            <a href="https://orcid.org/xxxxx">
			                                <i class="ai ai-orcid-square ai-fw"></i>
			                                ORCID
			                            </a>
			                        </li> -->
			    				</ul>
			    			</div>
			    		</div>
			    	</div>
			    	<article class="page" itemscope itemtype="http://schema.org/CreativeWork">
			    		<meta itemprop="headline" content="About me">
			    		<meta itemprop="description" content="About me">
			    		<div class="page__inner-wrap">
			    			<header>
			    				<h1 class="page__title" itemprop="headline">About me</h1>
			    			</header>
			    			<section class="page__content" itemprop="text">
			    				<p>
			    					Rui Liu is currently a Professor in National and Local Joint Engineering Research Center of
			    					Mongolian Intelligent Information Processing, Inner Mongolia University. Rui Liu received
			    					Ph.D degree from Inner Mongolia University, China in 2020 and Bachelor degree in Taiyuan
			    					University of Technology, ShanXi, China in 2014. From 2020 to 2022, he worked as a Research
			    					Fellow at the Department of Electrical and Computer Engineering, National University of
			    					Singapore, Singapore, working with Prof. <a href="https://www.colips.org/~eleliha/">Haizhou
			    						Li</a>. He was the recipient of the "Best Paper Award" at the 2021 International
			    					Conference on Asian Language Processing.
			    				</p>
			    				<p>
			    					His research papers are mainly published in top-tier NLP/ML/AI journals and conferences,
			    					including IEEE/ACM-TASLP, IEEE-TAC, Neural Networks, AAAI, ICASSP, COLING, INTERSPEECH, etc. 
								
			    				</p>


							<p><a href="https://ccs.imu.edu.cn/info/1152/4634.htm">Chinese Version Personal Official Website</a></p>
			    
			    				<h2 id="education-and-experience">Education and Experience</h2>
			    				<ul>
			    					<li>
			    						Jan 2022-Present, Professor
			    						</br><a href="http://www.mglip.com/">National and Local Joint Engineering Research
			    							Center of Mongolian Intelligent Information Processing</a>, Inner Mongolia
			    						University, China.
			    					</li>
			    					<li>
			    						Aug 2020-Jan 2022, Research Fellow (Advisor: Prof. <a
			    							href="https://www.colips.org/~eleliha/">Haizhou Li</a>)
			    						</br> <a href="https://cde.nus.edu.sg/ece/hlt/">HLT Lab</a>, National University of
			    						Singapore, Singapore.
			    					</li>
			    					<li>
			    						Aug 2019-Aug 2020, Visiting Ph.D. Student (Advisor: Prof. <a
			    							href="https://www.colips.org/~eleliha/">Haizhou Li</a>)
			    						</br> <a href="https://cde.nus.edu.sg/ece/hlt/">HLT Lab</a>, National University of
			    						Singapore, Singapore.
			    					</li>
			    					<li>
			    						Sep 2014-Aug 2020, Ph.D. Student (Advisor: Prof. <a
			    							href="https://www.imu.edu.cn/info/1023/2690.htm"> Guanglai Gao</a>)
			    						</br> Inner Mongolia University, Hohhot, China.
			    						</br> Research topic: Mongolian Text-to-Speech system <a
			    							href="http://mtts.mglip.com/">[DEMO]</a>.
			    					</li>
			    					<li>
			    						Sep 2010-June 2014, Undergraduate
			    						</br> Taiyuan University of Technology, ShanXi, China.
			    					</li>
			    
			    				</ul>
			    				 
			    				<h2 id="publication" class="page-header">Selected Publications <a
			    						href="https://scholar.google.com.sg/citations?user=B2t0J-IAAAAJ&amp;hl=zh-CN"
			    						target="_blank" rel="external">[Google Scholar]</a></h2>
			    				(#: equal contribution *: corresponding author)
<!-- 			    				<h3>Preprints</h3>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						<b>Controllable Accented Text-to-Speech Synthesis </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">To be submitted for possible journal publication <br></font>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.10804"
			    								target="_blank">[PDF]</a> <a
			    								href="https://speechdemo.github.io/caitts/">[DEMO]</a></font>
			    					</li>
			    					<li>
			    						<b>FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational
			    							Speech Synthesis </b> <br>
			    						Yifan Hu, <b>Rui Liu <sup>*</sup></b>, Guanglai Gao, Haizhou Li.<br>
			    						 <font color="#1B58B8">Submitted to ICASSP'2023 <br></font> 
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15360"
			    								target="_blank">[PDF]</a> <a
			    								href="https://walker-hyf.github.io/FCTalker/">[DEMO]</a></font> <a
			    							href="https://github.com/walker-hyf/FCTalker">[CODE]</a></font>
			    					</li>
			    				</ol> !-->
			    
			    				<h3>Journal papers</h3>
			    				<ol class="paper-list" id="grants">

								<li>
			    						<b>Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities </b> <br>
			    						<b>Rui Liu</b>, Haolin Zuo, Zheng Lian, Bjorn W. Schuller and Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE Transactions on Affective Computing
			    							(IEEE-TAC). 2024 <br></font>
			    						<font color="red">(Top journal, CAAI-A, IF=11.2)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/10474146/"
			    								target="_blank">[PDF]</a> <a href="https://github.com/ZhuoYulang/CIF-MMIN">[CODE]</a>
										
			    						</font>
			    					</li>


								<li>
			    						<b>Controllable Accented Text-to-Speech Synthesis with Fine and Coarse-Grained Intensity Rendering </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao and Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2024 <br></font>
			    						<font color="red">(Top journal, TH-CPL-A, IF=4.364)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/10487819"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/CTA-TTS/">[DEMO]</a>
			    						</font>
			    					</li>


								<li>
			    						<b>Text-to-Speech for Low-Resource Agglutinative Language with Morphology-Aware Language Model Pre-training </b> <br>
			    						<b>Rui Liu</b>, Yifan Hu, Haolin Zuo, Zhaojie Luo, Longbiao Wang and Guanglai Gao.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2024 <br></font>
			    						<font color="red">(Top journal, TH-CPL-A, IF=4.364)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/10379131"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/MAM-BERT/">[DEMO]</a>
			    						</font>
			    					</li>

								<li>
			    						<b>Multi-Space Channel Representation Learning for Mono-to-Binaural Conversion based Audio Deepfake Detection </b> <br>
			    						<b>Rui Liu</b>, Jinhua Zhang and Guanglai Gao.<br>
			    						<font color="#1B58B8">Information Fusion. 2024 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=18.6)</font><br>
			    						<font color="#1B58B8"><a href="https://www.sciencedirect.com/science/article/pii/S1566253524000356"
			    								target="_blank">[PDF]</a>
			    						</font>
			    					</li>


								
			    					<li>
			    						<b>Decoupling Speaker-Independent Emotions for Voice Conversion Via Source-Filter
			    							Networks </b> <br>
			    						Zhaojie Luo, Shoufeng Lin, <b>Rui Liu <sup>*</sup></b>, Jun Baba, Yuichiro Yoshikawa,
			    						Ishiguro Hiroshi.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2022 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9829916"
			    								target="_blank">[PDF]</a> <a href="https://zhaojiel.github.io/SFEVC/">[DEMO]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>Decoding Knowledge Transfer for Neural Text-to-Speech Training</b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2022 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9767637"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/MT-KD/">[DEMO]</a> <a
			    								href="./papers/TASLP2022-decoding.txt" target="_blank">[BIB]</a></font>
			    					</li>
			    					<li>
			    						<b>Multi-Stage Deep Transfer Learning for EmIoT-enabled Human-Computer Interaction</b>
			    						<br>
			    						<b>Rui Liu</b>, Qi Liu, Hongxu Zhu, Hui Cao.<br>
			    						<font color="#1B58B8">IEEE Internet of Things Journal. 2022 </font><br> </font>
			    						<font color="red">(Top journal, JCR Q1, IF=10.238)</font><br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9702532"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/IOT/">[DEMO]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>MonTTS: A Real-time and High-fidelity Mongolian TTS Model with Complete
			    							Non-autoregressive Mechanism (in Chinese) </b> <br>
			    						<b>Rui Liu</b>, Shyin Kang, Jingdong Li, Feilong Bao, Guanglai Gao.<br>
			    						<font color="#1B58B8">Journal of Chinese Information Processing. 2022 </font><br>
			    						</font>
			    						<font color="red">(CCF T1)</font><br>
			    						<font color="#1B58B8"><a href="http://jcip.cipsc.org.cn/CN/abstract/abstract3357.shtml"
			    								target="_blank">[PDF]</a> <a href="https://github.com/ttslr/MonTTS">[CODE]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>Emotional Voice Conversion: Theory, Databases and ESD </b> <br>
			    						Kun Zhou, Berrak Sisman, <b>Rui Liu</b>, Haizhou Li.<br>
			    						<font color="#1B58B8">Speech Communication. 2021 </font><br> </font>
			    						<font color="red">(Top journal, CCF-B, IF=2.723)</font><br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2105.14762"
			    								target="_blank">[PDF]</a> <a
			    								href="https://hltsingapore.github.io/ESD/demo.html">[DEMO]</a></font>
			    					</li>
			    					<li>
			    						<b>Expressive TTS Training with Frame and Style Reconstruction Loss </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2021 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9420276"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ttslr.github.io/Expressive-TTS-Training-with-Frame-and-Style-Reconstruction-Loss/">[DEMO]</a>
			    							<a href="./papers/TASLP2021-style.txt" target="_blank">[BIB]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>FastTalker: A Neural Text-to-Speech Architecture with Shallow and Group
			    							Autoregression </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Yixing Lin, Haizhou Li.<br>
			    						<font color="#1B58B8">Neural Networks. 2021 </font>
			    						<br>
			    						<font color="red">(JCR Q1, IF=9.657)</font>
			    						<br>
			    						<font color="#1B58B8"><a
			    								href="https://www.sciencedirect.com/science/article/pii/S0893608021001532"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ttslr.github.io/FastTalker/">[DEMO]</a> <a
			    								href="./papers/NN2021.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>Exploiting Morphological and Phonological Features to Improve Prosodic Phrasing for
			    							Mongolian Speech Synthesis </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Feilong Bao, Jichen Yang, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE/ACM Transactions on Audio, Speech, and Language Processing
			    							(IEEE/ACM-TASLP). 2021 <br></font>
			    						<font color="red">(Top journal, JCR Q1, IF=4.364)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/TASLP2020Mongolian.pdf">[PDF]</a> <a
			    								href="./papers/TASLP2020Mongolian.txt" target="_blank">[BIB]</a></font>
			    					</li>
			    					<li>
			    						<b>Modeling Prosodic Phrasing with Multi-Task Learning in Tacotron-based TTS </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">IEEE Signal Processing Letters. 2020 </font>
			    						<br>
			    						<font color="red">(JCR Q1, IF=3.201)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9166626"
			    								target="_blank">[PDF]</a> <a href="./papers/SPL2020.txt"
			    								target="_blank">[BIB]</a> <a href="https://ttslr.github.io/SPL2020/">[DEMO]</a>
			    						</font>
			    					</li>
			    				</ol>
			    
			    				<h3>Conference papers</h3>
			    				<ol class="paper-list" id="grants">


								<li>
			    						<b>Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling </b> <br>
			    						<b>Rui Liu</b>, Yifan Hu, Yi Ren, Xiang Yin, Haizhou Li.<br>
			    						<font color="#1B58B8">38th AAAI Conference on Artificial Intelligence (AAAI'2024).</font> <br>
			    						<font color="red">(Top conference, CCF-A)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/29833"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/walker-hyf/ECSS">[CODE & DEMO]</a></font>
			    					</li>
								
			    					<li>
			    						<b>Exploiting Modality-Invariant Feature for Robust Multimodal Emotion Recognition with
			    							Missing Modalities </b> <br>
			    						Haolin Zuo, <b>Rui Liu <sup>*</sup></b>, Jinming Zhao, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">2023 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2023).</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2210.15359"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/ZhuoYulang/IF-MMIN">[CODE]</a></font>
			    					</li>
			    					<li>
			    						<b>MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied
			    							Baseline </b> <br>
			    						Yifan Hu <sup>#</sup>, Pengkai Yin <sup>#</sup>, <b>Rui Liu <sup>*</sup></b>, Feilong
			    						Bao and Guanglai Gao.<br>
			    						<font color="#1B58B8">2022 International Conference on Asian Language Processing
			    							(IALP'2022) </font> <br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.10848"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/walker-hyf/MnTTS">[CODE]</a> <a
			    								href="http://mglip.com/corpus/corpus_detail.html?corpusid=20220819185345">[Application
			    								Entry]</a></font>
			    					</li>
			    					<li>
			    						<b>A Deep Investigation of RNN and Self-attention for the Cyrillic-Traditional Mongolian
			    							Bidirectional Conversion </b> <br>
			    						Muhan Na, <b>Rui Liu <sup>*</sup></b>, Feilong Bao and Guanglai Gao.<br>
			    						<font color="#1B58B8">29th International Conference on Neural Information Processing
			    							(ICONIP'2022) </font> <br>
			    						<font color="red">(CCF-C)</font> <br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2209.11963"
			    								target="_blank">[PDF]</a> </font>
			    					</li>
			    					<li>
			    						<b>Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven
			    							Deep Learning </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Björn Schuller, Guanglai Gao and Haizhou Li.<br>
			    						<font color="#1B58B8">23th Annual Conference of the International Speech Communication
			    							Association (INTERSPEECH'2022) </font> <br>
			    						<font color="red">(Top conference, CCF-C)</font> <br>
			    						<font color="#1B58B8"><a
			    								href="https://www.isca-speech.org/archive/interspeech_2022/liu22i_interspeech.html"
			    								target="_blank">[PDF]</a> <a
			    								href="https://github.com/ttslr/StrengthNet">[CODE]</a></font>
			    					</li>
			    					<li>
			    						<b>Alignment-Learning based single-step decoding for accurate and fast
			    							non-autoregressive speech recognition</b> <br>
			    						Yonghe Wang, <b>Rui Liu <sup>*</sup></b>, Feilong Bao, Hui Zhang, Guanglai Gao.<br>
			    						<font color="#1B58B8">2022 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2022).</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://ieeexplore.ieee.org/document/9746227"
			    								target="_blank">[PDF]</a> </font>
			    					</li>
			    					<li>
			    						<b>VisualTTS: TTS with Accurate Lip-speech Synchronization for Automatic Voice Over</b>
			    						<br>
			    						Junchen Lu, Berrak Sisman, <b>Rui Liu</b>, Mingyang Zhang, Haizhou Li.<br>
			    						<font color="#1B58B8">2022 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2022).</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/abs/2110.03342"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ranacm.github.io/VisualTTS-Samples/">[DEMO]</a></font>
			    					</li>
			    					<li>
			    						<b>Mongolian emotional speech synthesis based on transfer learning and emotional
			    							embedding </b> <br>
			    						Aihong Huang, Feilong Bao, Guanglai Gao, Yu Shan, <b>Rui Liu <sup>*</sup></b><br>
			    						<font color="red"><b>(Best Paper Award)</b></font><br>
			    						<font color="#1B58B8">2021 International Conference on Asian Language Information
			    							Processing (IALP'2021) </font> <br>
			    						<font color="#1B58B8"><a href="xx" target="_blank">[PDF]</a> </font>
			    					</li>
			    					<li>
			    						<b>Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion
			    							Discriminability </b> <br>
			    						<b>Rui Liu</b>, Berrak Sisman, Haizhou Li.<br>
			    						<font color="#1B58B8">22th Annual Conference of the International Speech Communication
			    							Association (INTERSPEECH'2021) </font> <br>
			    						<font color="red">(Top conference, CCF-C)</font> <br>
			    						<font color="#1B58B8"><a
			    								href="https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html"
			    								target="_blank">[PDF]</a> <a href="https://ttslr.github.io/i-ETTS/">[DEMO]</a>
			    							<a href="./papers/InterSpeech2021.txt" target="_blank">[BIB]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b> GraphSpeech: Syntax-aware Graph Attention Network for Neural Speech Synthesis </b>
			    						<br>
			    						<b>Rui Liu</b>, Berrak Sisman, Haizhou Li.<br>
			    						<font color="#1B58B8">2021 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2021), Oral.</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/pdf/2010.12423.pdf"
			    								target="_blank">[PDF]</a> <a
			    								href="https://ttslr.github.io/GraphSpeech/">[DEMO]</a> <a
			    								href="./papers/ICASSP2021.txt" target="_blank">[BIB]</a> <a
			    								href="https://www.bilibili.com/video/BV16U4y1t7cR/">[VIDEO]</a></font>
			    					</li>
			    					<li>
			    						<b>Seen and Unseen Emotional Style Transfer for Voice Conversion with a New Emotion
			    							Speech Dataset </b> <br>
			    						Kun Zhou, Berrak Sisman, <b>Rui Liu</b>, Haizhou Li.<br>
			    						<font color="#1B58B8">2021 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2021), Oral.</font> <br>
			    						<font color="red">(Top conference, CCF-B)</font>
			    						<br>
			    						<font color="#1B58B8"><a href="https://arxiv.org/pdf/2010.14794.pdf"
			    								target="_blank">[PDF]</a> <a href="./papers/ICASSP2021-Kun.txt"
			    								target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>Teacher-Student Training For Robust Tacotron-based TTS </b> <br>
			    						<strong>Rui Liu</strong>, Berrak Sisman, Jingdong Li, Feilong Bao, Guanglai Gao, Haizhou
			    						Li.<br>
			    						<font color="#1B58B8">2020 IEEE International Conference on Acoustics, Speech and Signal
			    							Processing (ICASSP'2020), Oral. <br>
			    							<font color="red">(Top conference, CCF-B) (With Travel Grant)</font>
			    						</font><br>
			    						<font color="#1B58B8"><a href="papers/ICASSP2020.pdf">[PDF]</a> <a
			    								href="./papers/ICASSP2020.txt" target="_blank">[BIB]</a> <a
			    								href="https://ttslr.github.io/ICASSP2020/">[DEMO]</a> <a
			    								href="https://youtu.be/D6gTTuiDdPU">[VIDEO]</a></font>
			    					</li>
			    					<li>
			    						<b>WaveTTS: Tacotron-based TTS with Joint Time-Frequency Domain Loss </b> <br>
			    						<strong>Rui Liu</strong>, Berrak Sisman, Feilong Bao, Guanglai Gao, Haizhou Li.<br>
			    						<font color="#1B58B8">The Speaker and Language Recognition Workshop 2020 (Odyssey'2020).
			    						</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/Odyssey2020.pdf">[PDF]</a> <a
			    								href="./papers/Odyssey2020.txt" target="_blank">[BIB]</a> <a
			    								href="https://ttslr.github.io/WaveTTS/">[DEMO]</a> <a href="xxx">[VIDEO]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>NUS-HLT System for Blizzard Challenge 2020 </b> <br>
			    						Yi Zhou, Xiaohai Tian, Xuehao Zhou, Mingyang Zhang, Grandee Lee, <strong>Rui
			    							Liu</strong>, Berrak Sisman, and Haizhou Li<br>
			    						<font color="#1B58B8">Joint Workshop for the Blizzard Challenge and Voice Conversion
			    							Challenge 2020 (BC'2020).</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/BC2020.pdf">[PDF]</a> <a
			    								href="./papers/BC2020.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>The IMU Speech Synthesis Entry for Blizzard Challenge 2019 </b> <br>
			    						<strong>Rui Liu</strong>, Jingdong Li, Feilong Bao and Guanglai Gao.<br>
			    						<font color="#1B58B8">Blizzard Challenge Workshop 2019 (BC'2019).</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/BC2019.pdf">[PDF]</a> <a
			    								href="./papers/BC2019.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>Improving Mongolian Phrase Break Prediction by Using Syllable and Morphological
			    							Embeddings with BiLSTM Model </b> <br>
			    						<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao, Hui Zhang and Yonghe Wang.<br>
			    						<font color="#1B58B8">19th Annual Conference of the International Speech Communication
			    							Association (INTERSPEECH'2018), Oral </font>
			    						<br>
			    						<font color="red">(Top conference, CCF-C)</font>
			    						<br>
			    						<font color="#1B58B8"><a
			    								href="https://www.isca-speech.org/archive_v0/Interspeech_2018/abstracts/1706.html">[PDF]</a>
			    							<a href="./papers/INTERSPEECH2018.txt" target="_blank">[BIB]</a>
			    						</font>
			    					</li>
			    					<li>
			    						<b>A LSTM Approach with Sub-word Embeddings for Mongolian Phrase Break Prediction </b>
			    						<br>
			    						<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao, Hui Zhang and Yonghe Wang.<br>
			    						<font color="#1B58B8">27th International Conference on Computational Linguistics
			    							(COLING'2018).</font>
			    						<br>
			    						<font color="red">(Top conference, CCF-B)</font><br>
			    						<font color="#1B58B8"><a href="https://aclanthology.org/C18-1207/">[PDF]</a> <a
			    								href="./papers/COLING2018.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    					<li>
			    						<b>End-to-End Mongolian Text-to-Speech System </b> <br>
			    						Jingdong Li, Hui Zhang, <strong>Rui Liu</strong>, Xueliang Zhang and Feilong Bao.<br>
			    						<font color="#1B58B8">11th International Symposium on Chinese Spoken Language Processing
			    							(ISCSLP'2018).</font><br>
			    						<font color="#1B58B8"><a href="papers/ISCSLP2018.pdf">[PDF]</a> <a
			    								href="./papers/ISCSLP2018.txt" target="_blank">[BIB]</a></font>
			    					</li>
			    					<li>
			    						<b>Mongolian Text-to-Speech System Based on Deep Neural Network </b> <br>
			    						<strong>Rui Liu</strong>, Feilong Bao, Guanglai Gao and Yonghe Wang.<br>
			    						<font color="#1B58B8">14th National Conference on Man-Machine Speech Communication
			    							(NCMMSC'2017), Oral.</font>
			    						<br>
			    						<font color="#1B58B8"><a href="papers/NCMMSC2017.pdf">[PDF]</a> <a
			    								href="./papers/NCMMSC2017.txt" target="_blank">[BIB]</a> </font>
			    					</li>
			    				</ol>
			    
			    				<h2>Projects</h2>
			    				<h3>Principal Investigator</h3>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						High-level Talents Introduction Project of Inner Mongolia University
			    						<br> No. 10000-22311201/002
			    						<br> 2022/05-2025/05
			    					</li>
			    					<li>
			    						Young Scientists Fund of the National Natural Science Foundation of China (NSFC)
			    						<br> No. 62206136
			    						<br> 2023/01-2025/12
			    					</li>
			    				</ol>
			    				<h3>Co-Principal Investigator</h3>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						Coming soon...
			    					</li>
			    				</ol>
			    
			    
			    				<h2>Talks</h2>
			    				<ol class="paper-list" id="grants">
			    					<li>
			    						<b>Title: </b><u>Mongolian Text-to-Speech Technology</u> （蒙古语语音合成技术）. <br />[<a
			    							href="./slides/多语种论坛-蒙古语TTS-v2.ppt" target="_blank">Slides</a>]
			    						[<a href="https://mp.weixin.qq.com/s/tdyJ0dygEwysYmGrfBFbxA" target="_blank">Video</a>]
			    						<br />
			    						<b>Organizer:</b> Chinese Association for Artificial Intelligence （CAAI） <br />
			    						<b>Date:</b> 20 Aug 2022
			    					</li>
			    					<li>
			    						<b>Title: </b><u>Emotion Intensity Research of Speech Synthesis</u> (语音合成中的情感强度建模研究).
			    						<br />[<a href="./slides/语音之家-情感强度建模-PPT.pdf" target="_blank">Slides</a>]
			    						[<a href="https://appzxw56sw27444.h5.xiaoeknow.com/v2/course/alive/l_6281eb95e4b0cedf38b26577?app_id=appzxw56sw27444&pro_id=&type=2&available=true&share_user_id=u_6278bea47e200_rUccGC7f14&share_type=5&scene=%E5%88%86%E4%BA%AB&is_redirect=1&share_scene=1&entry=2&entry_type=2002"
			    							target="_blank">Video</a>]
			    						<br />
			    						<b>Organizer:</b> SpeechHome （语音之家） <br />
			    						<b>Date:</b> 19 May 2022
			    					</li>
			    					<li>
			    						<b>Title: </b><u>Prosody and Emotion Modeling in End-to-End Speech Synthesis
			    						</u>（端到端语音合成中的韵律、情感建模研究）. <br />[<a href="./slides/CCF专委会报告--语音合成韵律情感-刘瑞.pdf"
			    							target="_blank">Slides</a>]
			    						[<a href="https://mp.weixin.qq.com/s/O5ok2Uh0Sd639C5oqtRo8A"
			    							target="_blank">Video</a>]<br />
			    						<b>Organizer:</b> CCF Professional Committee of Speech Dialogue and Auditory Processing
			    						<br />
			    						<b>Date:</b> 04 Dec 2021
			    					</li>
			    				</ol>
			    
			    				<h2>Activities</h2>
			    				<ol class="paper-list" id="grants">
			    					<li><b>Conference Reviewer</b>:<br>
									- ACL 2024 <br>
									- NAACL 2024  <br>
			    						- INTERSPEECH 2021/2022/2023/2024<br>
			    						- ICASSP 2021/2022/2023/2024<br>
			    						- Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020<br>
			    						- SLT 2022<br>
			    						- O-COCOSDA 2022<br>
			    					<li><b>Journal Reviewer</b>: <br>
			    						- IEEE/ACM Transactions on Audio, Speech, and Language Processing (IEEE/ACM-TASLP) <br>
									- IEEE Transactions on Neural Networks and Learning Systems (IEEE-TNNLS)<br>
			    						- IEEE Signal Processing Letters<br>
			    						- IEEE Internet of Things Journal (IEEE-IoTJ)<br>
									- Neural Nertworks<br>
									- NeuroComputing<br>
									- Speech Communication<br>
			    					<li><b>Professional Service</b>:
									<br>- Program Chair, <a
			    							href="www.ialp2024.org" target="_blank">IALP
			    							2024</a>, Hohhot, China
			    						<br>- Local Arrangement Co-chair, <a
			    							href="http://www.colips.org/conferences/cocosda2021/wp/" target="_blank">O-COCOSDA
			    							2021</a>, Singapore.
			    						<br>- Local Arrangement Co-chair, <a
			    							href="http://www.colips.org/conferences/iwsds2021/wp/" target="_blank">IWSDS
			    							2021</a>, Singapore
			    						<br>- Local Arrangement Co-chair, <a
			    							href="http://www.colips.org/conferences/sigdial2021/wp/" target="_blank">SIGDIAL
			    							2021</a>, Singapore
			    						<br>- Student Volunteer, ASRU 2019 (IEEE Automatic Speech Recognition and Understanding
			    						Workshop), Singapore
			    						<br>- Student Volunteer, NLPCC 2018 (7th CCF International Conference on Natural
			    						Language Processing and Chinese Computing), Hohhot, China.
			    						<br>- Program Committee Member, O-COCOSDA 2022.
			    				</ol>
			    
			    				<h2>Awards</h2>
			    				<ol class="paper-list" id="grants">
								<li>Apr 2023, <a href="https://www.acmturc.com/2022_bak/cn/mobile/new_star_award.html"
			    							target="_blank"> 2022 ACM China Rising Star</a> (Hohhot Chapter), ACM (Association for Computing Machinery) China Council
			    					<li>Dec 2021, Excellent Doctoral dissertation of Inner Mongolia Autonomous Region</li>
			    					<li>Dec 2021, IALP-2021 <font color="red"><b>Best Paper</b></font>
			    					</li>
			    					<li>July 2021, <a href="https://www.acmturc.com/2021/en/doctoral_thesis_award.html"
			    							target="_blank"> 2020 ACM China Doctoral Dissertation Award </a> (Hohhot Chapter),
			    						ACM (Association for Computing Machinery) China Council </li>
			    					<li>Sep 2020, Excellent Doctoral dissertation of Inner Mongolia University</li>
			    					<li>Feb 2020, ICASSP IEEE SPS Travel Grant </li>
			    					<li>Aug 2019, Research Scholarship of China Scholarship Council (CSC) </li>
			    					<li>Oct 2018, National scholarship for Doctoral students (top 2% students), Ministry of
			    						Education of P.R.China </li>
			    					<li>Oct 2018, Academic scholarship of Inner Mongolia autonomous region </li>
			    					<li>Oct 2017, National scholarship for Doctoral students (top 2% students), Ministry of
			    						Education of P.R.China </li>
			    					<li>Oct 2017, Academic scholarship of Inner Mongolia autonomous region </li>
			    					<li>Oct 2016, Academic scholarship of Inner Mongolia autonomous region </li>
			    					 
			    				</ol>
			    
			    
			    				<h2>Resource</h2>
			    				<ol class="resource-list" id="usefullinks">
			    					<li><a href="https://github.com/awesomedata/awesome-public-datasets" target="_blank">
			    							Awesome-Public-Datasets</a></li>
			    					<li><a href="https://www.aminer.cn/ranks/conf" target="_blank"> Aminer Ranking</a></li>
			    					<li><a href="./resource/中国计算机学会推荐国际学术会议和期刊目录-2019.pdf" target="_blank"> CCF Ranking</a></li>
			    					<li><a href="http://www.jdl.ac.cn/how_to_research/index1_1.htm#1" target="_blank"> How to do
			    							research</a></li>
			    					<li><a href="https://github.com/bighuang624/AI-research-tools"
			    							target="_blank">AI-research-tools</a></li>
			    					<li><a href="https://numbda.cs.tsinghua.edu.cn/~yuwj/TH-CPL.pdf" target="_blank">TH-CPL</a>
			    					</li>
			    					<li><a href="https://mp.weixin.qq.com/s/Hu_ozQG_uoYDLN0jQzUhDQ" target="_blank">CCF Journal
			    							Ranking</a></li>
			    					<li><a href="https://ccfddl.github.io/" target="_blank">CCF Deadlines</a></li>
			    				</ol>
			    				
<!-- 			    				<h2>More about Me</h2> -->
			    			</section>
			    			<footer class="page__meta"></footer>
			    		</div>
			    	</article>
			    </div>
				<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5bbihrdqns5&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
			</div>
			
			<div class="module" id="module2" style="display: none;">
			    <div class="block">
			    		<span>Principal Investigator</span>
			    		<div class="people_list">
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/RuiLiu_1.jpg"/>
			    				</div>
			    				<span>Rui Liu <br/> (刘瑞)</span><br/> <br/>
			    				<!-- <span><underline><a href="https://ttslr.github.io/index_ruiliu.html"  onclick="showModule(1)">Personal Homepage</a></underline></span> -->
			    				<span><underline><a onclick="showModule(1)">Personal Homepage</a></underline></span>
			    				<!-- <span>Rui Liu is a professor in Department of Computer Science at Inner Mongolia University of China, who is leading the S2LAB.</span> -->
			    			</div>
			    		</div>
			    	</div>
			    	
			    	<!-- 博士 -->
			    	<div class="block">
			    		<span>PhD Student</span>
			    		<span>19级</span>
			    		<div class="people_list">
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/nmh.jpg"/>
			    				</div>
			    				<span>Muhan Na <br/>(娜木汗) '19<br/>[蒙古文自然语言处理]</span>
			    				
			    			</div>
			    		</div>
			    		<span>22级</span>
			    		<div class="people_list">
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/zhl.jpg"/>
			    				</div>
			    				<span>Haolin Zuo <br/> (左昊麟) '22<br/>[多模态情感识别]</span> <br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 英国帝国理工、中科院自动化所、启元实验室 联合指导)</b></span>
			    			</div>
			    		</div>
			    		<span>23级</span>
			    		<div class="people_list">
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/hyf.jpg"/>
			    				</div>
			    				<span>Yifan Hu <br/> (胡一帆) '23<br/>[对话语音合成]</span><br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 新加坡字节跳动 联合指导)</b></span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/myx.jpg"/>
			    				</div>
			    				<span>Yuxuan Ma <br/> (麻宇轩) '23<br/>[多模态意图识别]</span>
			    			</div>
			    		</div>
			    	</div>
			    	
			    	<!-- 硕士 -->
			    	
			    	<div class="block">
			    		<span>Master Student</span>
			    		 
			    		<span>22级</span>
			    		<div class="people_list" style="justify-content: flex-start;">
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lb.jpg"/>
			    				</div>
			    				<span>Bin Liu <br/> (刘彬) '学硕<br/>[韵律预测]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/zjh.jpg"/>
			    				</div>
			    				<span>Jinhua Zhang <br/> (张锦华) '学硕<br/>[语音鉴伪]</span><br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 香港中文大学（深圳）联合指导)</b></span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lh.jpg"/>
			    				</div>
			    				<span>Huan Liu <br/> (刘欢) '学硕<br/>[语音情感识别]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/xjt.jpg"/>
			    				</div>
			    				<span>Jiatian Xi <br/> (席嘉甜) '专硕<br/>[语音编辑]</span><br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 浙江大学 联合指导)</b></span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/mzn.jpg"/>
			    				</div>
			    				<span>Zening Ma <br/> (马泽宁) '专硕<br/>[自监督预训练模型]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/lkl.jpg"/>
			    				</div>
			    				<span>Kailin Liang <br/> (梁凯麟) '专硕<br/>[语音情感迁移]</span>
			    			</div>
			    			
			    			 
			    			
			    		</div>
			    		<span>23级</span>
			    		<div class="people_list" style="justify-content: flex-start;">
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/gf.jpg"/>
			    				</div>
			    				<span>Pu Gao <br/> (高溥) '学硕<br/>[Speech Editing]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/zy.jpg"/>
			    				</div>
			    				<span>Yuan Zhao <br/> (赵源) '学硕<br/>[Video Dubbing]</span>
			    			</div>
			    		
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/wzy.jpg"/>
			    				</div>
			    				<span>Zhaoyang Wang <br/> (王召阳) '专硕<br/>[Talking Head Generation]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/xhh.jpg"/>
			    				</div>
			    				<span>Huhong Xian <br/> (鲜鹄鸿) '专硕<br/>[ADD]</span>
			    			</div>
			    			
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/hsw.jpg"/>
			    				</div>
			    				<span>Shuwei He <br/> (何树伟) '专硕<br/>[Speech Synthesis]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/gsl.jpg"/>
			    				</div>
			    				<span>Shaoli Ge <br/> (戈绍丽) '专硕<br/>[SNN]</span><br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 上海交通大学 联合指导)</b></span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/jzq.jpg"/>
			    				</div>
			    				<span>Zhenqi Jia <br/> (贾真琦) '专硕<br/>[Speech]</span>
			    			</div>
			    			
			    			<div class="people">
			    				<div class="people_header">
			    					<img src="assets/S2Group_members/yhy.jpg"/>
			    				</div>
			    				<span>Hongyu Yuan <br/> (袁宏宇) '专硕<br/>[LLM]</span><br/><br/><br/>
			    				<span  style="color: #015293;"><b>(与 华南理工大学 联合指导)</b></span>
			    			</div>
			    			
			    		</div>
			    		
						<span>24级</span>
						<div class="people_list" style="justify-content: flex-start;">
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/cwk.jpg"/>
								</div>
								<span>Wenkai Cheng <br/> (程文凯) '学硕<br/>[暂无研究方向]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/lhy.jpg"/>
								</div>
								<span>Guiyao Liu <br/> (刘桧耀) '学硕<br/>[暂无研究方向]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/dongruilin.jpg"/>
								</div>
								<span>Ruilin Dong <br/> (董瑞霖) '学硕<br/>[暂无研究方向]</span>
							</div>
						
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/mt.jpg"/>
								</div>
								<span>Tao Ma <br/> (马涛) '专硕<br/>[暂无研究方向]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/zyj.jpg"/>
								</div>
								<span>Yingjie Zhao <br/> (赵英杰) '专硕<br/>[暂无研究方向]</span>
							</div>
							
							<div class="people">
								<div class="people_header">
									<img src="assets/S2Group_members/louyi.jpg"/>
								</div>
								<span>Yi Lou <br/> (娄艺) '专硕<br/>[暂无研究方向]</span>
							</div>
							
						</div>
						
						
						
			    	</div>
			    	
			    	<!-- 校友 -->
			    	<!-- <div class="block">
			    		<span>Alumni</span>
			    		<div class="people_list">
			    		</div>
			    	</div> -->
			    	
			</div>
			
			<div class="module" id="module3" style="display: none;">
				<!-- <div class="block" style="text-align: center;">Coming Soon...</div> -->
				<div class="block">
					<div class="collaborators-list">
					    <div class="collaborator">
					        <img src="assets/collaborators/lihaizhou.png" alt="Collaborator 1" class="collaborator-image">
					        <div class="collaborator-info">
					            <h2>李海洲</h2>
					            <p>李海洲，香港中文大学（深圳）数据科学学院执行院长、新加坡工程院院士、教育部长江学者，同时他也是新加坡国立大学客座教授。他曾于2003年至2016年担任新加坡科技研究局通信与资讯研究院首席科学家和研究总监。李教授是IEEE Fellow、ISCA Fellow、AAIA Fellow，曾任顶级期刊IEEE/ACM Transactions on Audio、Speech and Language Processing主编 (2015-2018年）。他也曾是多个国际大型学术会议的大会主席，包括ACL 2012、INTERSPEECH 2014、ICASSP 2022，以及EMNLP 2023的local chair。</p>
								<p>主页：[<a href="https://colips.org/~eleliha/">https://colips.org/~eleliha/</a>](<a href="https://colips.org/~eleliha/">https://colips.org/~eleliha/</a>)</p>
							</div>
					    </div>
					    <div class="collaborator">
					        <img src="assets/collaborators/Schuller.png" alt="Collaborator 2" class="collaborator-image">
					        <div class="collaborator-info">
					            <h2>Björn W. Schuller</h2>
					            <p>Björn W. Schuller：博士，伦敦帝国理工学院人工智能正教授和GLAM负责人，大学教授，南京东南大学客座教授，哈工大(中国)永久客座教授。他是IEEE/BCS/ISCA Fellow，AAAC名誉主席和ACM高级会员。他(合著)了1,000多篇出版物，是《数字健康前沿》的现场主编，也是IEEE情感计算汇刊的主编。他的30+奖项包括2015年被世界经济论坛评为40位40岁以下杰出科学家之一。他曾在15+个欧洲项目中担任协调员/PI，是ERC启动和DFG Reinhart-Koselleck受赠人，以及巴克莱、GN、华为、Informeis或三星等公司的顾问。</p>
								<p>主页：[<a href="http://www.schuller.one/">http://www.schuller.one/</a>](<a href="http://www.schuller.one/">http://www.schuller.one/</a>)</p>
							</div>
					    </div>
					    <!-- 可以根据需要继续添加更多合作者 -->
					</div>
				</div>
				
			</div>
		</main>
		<footer class="pc_footer">
			<div class="element">
				<i class="fa fa-envelope-open-o" aria-hidden="true" /></i>
				<span>liurui_imu @163.com</span>
			</div>
			
			<div class="element">
				<i class="fa fa-phone" aria-hidden="true" /></i>
				<span>+86 16647162610</span>
			</div>
			
			<div class="element">
				<i class="fa fa-map-marker" aria-hidden="true" /></i>
				<span> 503 Room, School of Computer science, <br/>Inner Mongolia University (010021)</span>
			</div>
			
			<div class="element">
				<i class="fa fa-map" aria-hidden="true" /></i>
				<span>Hohhot, China</span>
			</div>
		</footer>
		<script src="./js/main.min.js"></script>
		<script>
			(function(i, s, o, g, r, a, m) {
				i['GoogleAnalyticsObject'] = r;
				i[r] = i[r] || function() {
						(i[r].q = i[r].q || []).push(arguments)
					},
					i[r].l = 1 * new Date();
				a = s.createElement(o),
					m = s.getElementsByTagName(o)[0];
				a.async = 1;
				a.src = g;
				m.parentNode.insertBefore(a, m)
			})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
			ga('create', '', 'auto');
			ga('send', 'pageview');
			
		
			
			// 为第一个module 增加 加粗效果
			document.addEventListener('DOMContentLoaded', function() {
			    // 获取第一个菜单项元素
			    var firstMenuItem = document.querySelector('.masthead__menu-item');
			
			    // 为第一个菜单项添加 masthead__menu-item--lg 类
			    if (firstMenuItem) {
			        firstMenuItem.classList.add('masthead__menu-item--lg');
			    }
			});
		
			
		
			function showModule(moduleNumber) {
			    // 获取所有模块元素
			    var modules = document.querySelectorAll('.module');
			
			    // 隐藏所有模块
			    modules.forEach(function(module) {
			        module.style.display = 'none';
			    });
			
			    // 显示指定模块
			    var targetModule = document.getElementById('module' + moduleNumber);
			    if (targetModule) {
			        targetModule.style.display = 'block';
			    }
			
			    // 清除所有菜单项的活动状态
			    var menuItems = document.querySelectorAll('.masthead__menu-item');
			    menuItems.forEach(function(item) {
			        item.classList.remove('masthead__menu-item--lg');
			    });
			
			    // 将点击的 li 元素添加 masthead__menu-item--lg 类
			    var targetMenuItem = document.querySelector('.masthead__menu-item:nth-child(' + moduleNumber + ')');
			    if (targetMenuItem) {
			        targetMenuItem.classList.add('masthead__menu-item--lg');
			    }
			}
		
		</script>
	</body>
</html>
